{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Function and equation for later use\n",
    "$$ p(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y), given y=0, mu=0, and sigma=0.1 is 3.989422804014327\n"
     ]
    }
   ],
   "source": [
    "def gaussian_pdf(y, mu, sigma):\n",
    "    # Constant calculation\n",
    "    constant = 1 / (sigma * np.sqrt(2 * np.pi))\n",
    "    # Exponential calculation\n",
    "    exponent = np.exp(-((y - mu) ** 2) / (2 * sigma ** 2))\n",
    "    \n",
    "    return constant * exponent\n",
    "\n",
    "mu = 0          # mean\n",
    "sigma = 0.1     # standard deviation\n",
    "y = 0           # value to calculate the probability density for\n",
    "\n",
    "p_y = gaussian_pdf(y, mu, sigma)\n",
    "print(f\"P(y), given y={y}, mu={mu}, and sigma={sigma} is {p_y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Within Chapter Exercises:**\n",
    "- 3.1\n",
    "- 3.2\n",
    "- 3.3\n",
    "- 3.5\n",
    "\n",
    "**End of Chapter Problems:**\n",
    "\n",
    "- 3.1\n",
    "- 3.2\n",
    "- 3.3\n",
    "- 3.5\n",
    "\n",
    "**Question about Kording and Wolpert (2004): \"Bayesian integration in sensorimotor learning\":**\n",
    "\n",
    "1.  The model the authors use is essentially the same one we learned about in Chapter 3. Make sure this is clear to you by translating their variable names to the ones used in the textbook. That is, what takes the place of the stimulus, s_hyp, and what do they use for the measurement, x_obs? Do the same for the variances.\n",
    "2.  Note that x_estimated (at the top of the fourth page of the article pdf) is the posterior mean estimate. However, their expression looks different than the one in the book. Convince yourself (i.e., prove) that the form used by Kording & Wolpert is mathematically equivalent to the expression from Chapter 3 (Eqns 3.24, 3.28).\n",
    "\n",
    "**Bonus part 1:** The authors make a technically incorrect statement following the first equation in the \"Bayesian estimation\" section of the Methods.  The incorrect phrase they use is \"where p(x_sensed|x_true) is the likelihood of perceiving x_sensed\". What's wrong with this phrase and how should it be written?\n",
    "\n",
    "**Bonus part 2:** This one is a little more challenging. Feel free to take a pass on it. Immediately following the equation for the posterior mean estimate, the authors state that \"we can estimate the uncertainty in the feedback $\\sigma_\\text{sensed}$ by linear regression from Fig. 2a. Explain this, meaning: put it into the form of a regression model, explain what is known/unknown, where each variable comes from, and how you would solve? (Hint: The equation for $x_\\text{estimated}$ can be thought of as a univariate regression--that is,  a model with only a slope coefficient and no bias term.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Within Chapter Exercises  \n",
    "### Exercise 3.1  \n",
    "**If one substitutes y = μ and σ = 0.1 in Eq. (3.2), one finds p(y) = 3.99. How can a probability be larger than 1? If the answer to this question is not immediately clear, read Section B.5.4, on the difference between probability mass and density functions.**  \n",
    "\n",
    "Equation 3.2: \n",
    "$$ p(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "This is because it is not a probability, it is a probability density. The probability at any given value of $y$ is 0, however, the density is not. In order to calculate the probability, we would need to integrate over some range of this probability density. A probability density will have an integral equal to one, but the density at any given point can be greater or less than one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2  \n",
    "**Prove that statement mathematically.**\n",
    "\n",
    "From the textbook:  \n",
    "The expression of the posterior mean is of the form:\n",
    "$$wx_{obs} + (1-w)\\mu$$\n",
    "$$\\textrm{where}$$\n",
    "$$w = \\frac{J}{J+J_s}$$\n",
    "Since $J$ and $J_s$ are both non-negative, w is a number between 0 and 1.\n",
    "\n",
    "Answer:  \n",
    "Let's start with the two extremes. When $w=0$, the posterior mean is equal to $\\mu$, while when $w=1$, the posterior mean is equal to $x_{obs}$. This effectively sets our upper and lower bounds for the posterior mean, and therefore, the mean will always lie between these upper and lower bounds, which are equal to $x_{obs}$ and $\\mu$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3   \n",
    "**In the special case that prior and likelihood have the same variance (σ = σs), compute the mean of the posterior.**   \n",
    "\n",
    "When the variance is equal between the prior and the likelihood, the posterior mean will be exactly in the middle. The posterior mean is determined by the weights given to the mean of the likelihood and prior, and these weights are $w$ and $1-w$, where $w=\\frac{J}{J+J_s}$. Since the variance is equal, the precision ($J$) is also equal, which makes $w=0.5$. Therefore, the posterior mean can be calculated as:\n",
    "$$0.5x_{obs} + (1-0.5)\\mu$$\n",
    "Therefore, the posterior mean is the just the average of the prior and likelihood mean, and would fall right in the middle of the likelihood mean and prior mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5   \n",
    "\n",
    "**What is the variance of the posterior in the special case that σ = σs? What are the mean and the variance of the posterior when σ σs is very large or very small? Interpret your results.**   \n",
    "\n",
    "Since:\n",
    "$$\\sigma_{post}^2 = \\frac{1}{\\frac{1}{\\sigma^2}+\\frac{1}{\\sigma_s^2}}$$\n",
    "$$\\textrm{and}$$ \n",
    "$$\\sigma^2 = \\sigma_s^2$$\n",
    "$$\\sigma_{post}^2 = \\frac{1}{\\frac{2}{\\sigma}}$$\n",
    "Which then can be simplified to:\n",
    "$$\\sigma_{post}^2 = \\frac{\\sigma^2}{2}$$\n",
    "Therfore, the posterior variance will be equal to exaclty half of the prior or likelihood variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of chapter problems  \n",
    "### Problem 3.1\n",
    "**Let s be the stimulus of interest, x the measurement, ps(s) the stimulus distribution, and px|s(x|s) the measurement distribution.  \n",
    "a) Write down the posterior distribution over hypothesized stimulus s, given an observed measurement xobs.   \n",
    "b) Which of the terms in your expression is called the likelihood function?   \n",
    "c) What is the difference between the likelihood function and the measurement distribution?**\n",
    "\n",
    "**Answers**  \n",
    "a.\n",
    "$$P(s|x_{obs}) = \\frac{P(x_{obs}|s)P(s)}{p(x_{obs})}$$\n",
    "b.  \n",
    "The likelihood in this case would be $P(x_{obs}|s)$  \n",
    "\n",
    "c.  \n",
    "The measurement distribution is the distribution of the measurement $x$ for a given stimulus ($s$) value. It is a conditional probability regarding the probability of the measurment given the stimulus, when that stimulus is repeated many times. This would be an example of an objective probability, and is related to the physical world.\n",
    "The likelihood function is also a conditional probability, but it represents the oberserver's belief about the stimulus given the measurement. This would be an example of a subjective probability.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
