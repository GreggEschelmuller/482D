{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8caf9e2f-db7b-49a9-b7ef-1edf33c2e823",
   "metadata": {},
   "source": [
    "# Problem set 5\n",
    "\n",
    "- 5.1  \n",
    "- 5.2  \n",
    "- 5.5 (only parts a through c; for part b, there is no need to include $\\mu$ in your answer despite the instructions otherwise)  \n",
    "- 5.6 (hint for part d: the absolute value of w for which the variance is minimal will depend on the relative magnitudes of $sigma_1$ and $sigma_2$; however, the general solution for the optimal w can be defined strictly in terms of $sigma_1$ and $sigma_2$. Try different values of sigma1 and sigma2 to get some intuition. For calculus fans, you can also derive an analytical solution, but this is not required.)  \n",
    "- 5.7  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5fce4-56d6-4ec2-9010-810732af705b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1\n",
    "**An observer combines conditionally independent cues A and B with Gaussian measurement noise. When B becomes more reliable, the observer’s estimate will:**  \n",
    "a) shift towards A  \n",
    "b) shift towards B  \n",
    "c) stay the same  \n",
    "d) there is insufficient information to answer \n",
    "\n",
    "**Answer**\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a478888b-4367-4271-88ba-b26ddcbefcd5",
   "metadata": {},
   "source": [
    "### 5.2\n",
    "**True or false? Explain.**  \n",
    "a) In the cue combination model of this chapter, the measurements are assumed to be independent of each other.   \n",
    "b) Conflicts between two measurements generated by a single source rarely occur in real-world perception.\n",
    "\n",
    "**Answer**  \n",
    "a. False. They are assumed to be conditionally independent, not completley independent.   \n",
    "b. False. Due to the noise within our sensory systems and that each sensory system measures different aspects of the real world, this is very common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905033ba-5bc2-4be4-a3ef-1092cdc393d8",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.5\n",
    "**This problem builds on Section 5.5 about evidence accumulation. An observer infers a stimulus s from a sequence of measurements, $x_{obs,1}$, $x_{obs,2}$, . . . , $x_{obs,T}$, made on a single trial. The stimulus distribution is Gaussian with mean $\\mu$ and variance $\\sigma_s^2$ . The distribution of the $t^{th}$ measurement, $p(x_t|s)$, is Gaussian with mean s and variance $\\sigma_s$ (identical for all measurements); we assume conditional independence.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d660d-0d89-49f3-89a7-1405314186cc",
   "metadata": {},
   "source": [
    "**a. What are the mean and variance of the posterior? You may start with the equations in Section 5.4.**  \n",
    "\n",
    "$$\\hat{s}_PM = \\mu_{post} = \\frac{ J_s\\mu+J_1x_{obs,1}+J_2x_{obs,2}...+J_Tx_{obs,T}}{J_s+J_1+J_2...+J_T}$$\n",
    "\n",
    "$$\\hat{s}_PM =\\mu_{post} = \\frac{J_s\\mu+\\sum{J_tx_{obs,t}}}{J_s + \\sum{J_t}}$$\n",
    "\n",
    "Since each measurement has an identical variance, we know that $J_t=\\frac{1}{\\sigma^2}$ for all of our $x_{obs}$.\n",
    "So $\\mu_{post}$ can be generalized to the following\n",
    "\n",
    "$$\\mu_{post} = \\frac{\\sum_{t=1}^{T}J_tx_{obs}}{\\sum_{t=1}^{T}J_t}$$\n",
    "or\n",
    "$$\\mu_{post} = \\frac{\\sum_{t=1}^{T}J_tx_{obs}}{T*J_t}$$\n",
    "Further simplified to:\n",
    "$$\\mu_{post} = \\frac{J_t\\sum_{t=1}^{T}x_{obs}}{T*J_t}$$\n",
    "And lastly\n",
    "$$\\mu_{post} = \\frac{\\sum_{t=1}^{T}x_{obs}}{T}$$\n",
    "\n",
    "\n",
    "\n",
    "Equation 5.28 is:\n",
    "$$\\sigma_{post}^2 = \\frac{1}{J_s + \\sum_{i=1}^{N} J_i}$$\n",
    "Which can be simplified to:\n",
    "$$\\sigma_{post}^2 = \\frac{1}{\\sum_{t=1}^{T}J_t}$$\n",
    "$$\\sigma_{post}^2 = \\frac{1}{T*J_t}$$\n",
    "which can be finally simplified to:\n",
    "$$\\sigma_{post}^2 = \\frac{\\sigma_s^2}{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd642723-05c2-4c01-8ad4-dca3b3b79396",
   "metadata": {},
   "source": [
    "**b. For a given stimulus s, we define relative bias as the difference between the mean PME and s itself, divided by the difference between the mean of the stimulus distribution and s. Derive an expression for relative bias in terms of μ, σ , σs, and T . Simplify the expression as much as you can.**\n",
    "\n",
    "$\\mu:$ mean of stimulus distribution  \n",
    "$\\sigma_s^2:$ = Variance of the stimulus distribution  \n",
    "$s:$ mean of the measurment distribution (actual value of stimulus)  \n",
    "$\\sigma^2:$ = Variance of the measurement distribution  \n",
    "$T:$ the number of measurments  \n",
    "$\\mu_{post}:$ the PME\n",
    "\n",
    "\n",
    "$$bias = \\frac{PME - s}{\\mu - s}$$\n",
    "$$bias = \\frac{\\frac{\\sum_{t=1}^{T}x_{obs}}{T} - s}{\\mu - s}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec19f9-9554-44ae-84f4-e0e9448d8da8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Interpret the expression in (b): explain intuitively how the dependencies on the variables make sense**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d8c65-4444-49a8-a1e3-504887216771",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.6\n",
    "**In this problem, we examine suboptimal estimation in the context of cue combination. Suppose an observer estimates a stimulus s from two conditionally independent, Gaussiandistributed measurements, xobs,1 and xobs,2. The prior is flat.**   \n",
    "\n",
    "**a. We start with a reminder of optimal estimation. Express the PME in terms of the measurements.**   \n",
    "\n",
    "$$PME = \\frac{J_1x_{obs,1}+J_sx_{obs,2}}{J_1 + J_2}$$\n",
    "\n",
    "**b. What is the variance of the PME across trials?**\n",
    "\n",
    "$$\\sigma = \\frac{1}{J_1 + J_2}$$\n",
    "\n",
    "**c. Now suppose the observer uses an estimator of the form $\\hat{s} = wx_{obs,1} + (1-w)x_{obs,2}$, where w can be any constant. Show that this estimate is unbiased (just like the PME); this means that the mean of the estimate for given s is equal to s .**  \n",
    "\n",
    "The expected value of $\\hat{s}$ is:\n",
    "\n",
    "$E[\\hat{s}] = w E[x_{obs,1}] + (1 - w) E[x_{\\text{obs},2}]$\n",
    "\n",
    "Since we are assuming that the observation woud be centered on the true stimulus value, we can get the following:\n",
    "\n",
    "$E[\\hat{s}] = w s + (1 - w) s$\n",
    "\n",
    "$E[\\hat{s}] = s(w + 1 - w)$\n",
    "\n",
    "$E[\\hat{s}] = s$\n",
    "\n",
    "**d. What is the variance of this estimate as a function of w? Plot this function. At which value of w is it minimal, and does this value make sense? State your final conclusion in words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5ecfd-b971-4371-89e3-22e3716ffca9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Part D: Variance of the Estimate\n",
    "\n",
    "Now, we need to find the variance of \\(\\hat{s}\\) as a function of \\(w\\), denoted as \\(Var(\\hat{s})\\). The variance of \\(\\hat{s}\\) is given by:\n",
    "\n",
    "\\[ Var(\\hat{s}) = Var(w x_{\\text{obs},1} + (1 - w) x_{\\text{obs},2}) \\]\n",
    "\n",
    "Assuming \\(x_{\\text{obs},1}\\) and \\(x_{\\text{obs},2}\\) are independent, the variance can be expressed as:\n",
    "\n",
    "\\[ Var(\\hat{s}) = w^2 Var(x_{\\text{obs},1}) + (1 - w)^2 Var(x_{\\text{obs},2}) \\]\n",
    "\n",
    "Let's denote \\(Var(x_{\\text{obs},1}) = \\sigma_1^2\\) and \\(Var(x_{\\text{obs},2}) = \\sigma_2^2\\). Then,\n",
    "\n",
    "\\[ Var(\\hat{s}) = w^2 \\sigma_1^2 + (1 - w)^2 \\sigma_2^2 \\]\n",
    "\n",
    "To plot this function and find the value of \\(w\\) that minimizes \\(Var(\\hat{s})\\), and then discuss if this value makes sense, we need to compute this variance with respect to different values of \\(w\\), and find its minimum.\n",
    "\n",
    "Let's proceed with the computation and plotting.\n",
    "\n",
    "The plot of the variance of the estimate \\(\\hat{s}\\) as a function of \\(w\\) shows that the variance changes with different values of \\(w\\). The minimum variance achieved is approximately \\(0.50005\\), which occurs at \\(w \\approx 0.495\\). \n",
    "\n",
    "This result suggests that the optimal weight \\(w\\) to minimize the variance of the estimate, given identical variances for both measurements (\\(\\sigma_1^2 = \\sigma_2^2\\)), is close to \\(0.5\\). This makes intuitive sense because, when both measurements are equally reliable (having the same variance), giving them equal weight (\\(w = 0.5\\)) in the estimation process is optimal. This balances the contribution of both measurements to the final estimate, minimizing the estimate's variance while maintaining its unbiasedness.\n",
    "\n",
    "In real-world cue combination, this outcome aligns with the principle of optimally integrating multiple sources of information to improve the accuracy of our estimates. When both cues (measurements) are equally reliable, relying on them equally is the best strategy to minimize the error in our estimates. This conclusion reinforces the fundamental insight from cue integration theory that the brain (or any estimator) should weigh multiple sources of information in proportion to their reliability to achieve the most accurate perception or estimation possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da1dee-22b4-46a4-b55c-3c521685358e",
   "metadata": {},
   "source": [
    "### 5.7 \n",
    "**In Chapters 3 and the present chapter, we were able to derive analytical expressions for the posterior distribution and the response distribution. For more complex psychophysical tasks (e.g. later in this book), however, analytical solutions often do not exist but we can still use numerical methods. To gain familiarity with such methods, we will work through the cue combination model in this chapter using numerical methods. We assume that the experimenter introduces a cue conflict between the auditory and the visual stimulus: s1 = 5 and s2 = 10. The standard deviations of the auditory and of the visual noise are σ1 = 2 and σ2 = 1, respectively. We assume a flat prior over s.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
